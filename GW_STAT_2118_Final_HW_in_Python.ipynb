{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final HW GW STAT 2118 in Python\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The purpose of this Jupyter notebook is to demonstrate how to run regression analysis in Python which does seem to be currently the most popular language for data science in industry (especially in tech). I could see something like this being used in a programming-heavy course on regression (or perhaps an undergraduate intro to data science course). \n",
    "\n",
    "## The outline of this notebook performs the same regression analysis in 3 different ways. \n",
    "\n",
    "1. The first is with the Python package __numpy__ for scientific computing. This can be used for all manipulations with matrices, linear algebra computations, among others. The functions are similar to how you would perform regression in R physically showing all the matrix manipulations. It has the advantage over SAS in that it allows the students to see what's happening under the hood in terms of matrix manipulations. In my experience as a teaching assistant for the undergraduate regression course, several students don't seem to understand that matrix multiplication works the same exact way no matter the dimensions of the matrices. They understand the examples that have 4 or 5 data points, but sometimes when asked about how to do things with 100 data points, they don't quite understand that it's done in the exact same way. \n",
    "\n",
    "2. The second is with the Python package __statsmodels__ for a more packaged rigorous statistical analysis similar to what you'd expect from R. The output from the OLS method in this package is very similar to the output from the R function summary(lm(y~., data=df)). This is probably the most similar functionality you'd get from PROC REG in SAS in that it just spits out everything you need without knowing exactly what Python is doing behind the scenes. \n",
    "\n",
    "3. The third is with the Python package __sci-kit learn (sklearn)__ for machine learning/predictive modeling. This package makes it very easy to implement machine learning models, but it does lack in rigorous statistical analysis. You can get a few validation metrics such as R^2 but not much else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One disadvantage with using Python is I haven't found a ready-to-go method for model selection like forward selection, backward elimination, stepwise regression, or subset regression. I've coded myself backward elimination below, which could just be used. Or perhaps in a more programming heavy regression course in the future, it would be a good homework problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Final HW STAT 2118\n",
    "\n",
    "# In Python, we need to import certain packages to do mathematical and data manipulations as well as visualiztion.\n",
    "\n",
    "import numpy as np # for working with matrices and doing linear algebra (think of np as a nickname for numpy)\n",
    "import pandas as pd # for working with data frames (think of pd as a nickname for pandas)\n",
    "import scipy # for working with specific mathematical and statistical functions\n",
    "import sklearn # for working with machine learning models\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import statsmodels.api as sm # for doing more rigorous statistical analysis similar to R\n",
    "\n",
    "# for visualizing inside this Jupyter Notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading in data file as a data frame\n",
    "data = pd.read_csv('C:/Users/Samuel/Documents/GW/Regression Analysis/class - fall 2018.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID         int64\n",
       "HW1       object\n",
       "HW2       object\n",
       "HW3       object\n",
       "MT1      float64\n",
       "HW4       object\n",
       "HW5       object\n",
       "HW6       object\n",
       "MT2      float64\n",
       "HW7       object\n",
       "Final    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes # look at types of data in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>HW1</th>\n",
       "      <th>HW2</th>\n",
       "      <th>HW3</th>\n",
       "      <th>MT1</th>\n",
       "      <th>HW4</th>\n",
       "      <th>HW5</th>\n",
       "      <th>HW6</th>\n",
       "      <th>MT2</th>\n",
       "      <th>HW7</th>\n",
       "      <th>Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>28.00</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>18.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>37.50</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>14.5</td>\n",
       "      <td>36.5</td>\n",
       "      <td>20</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>24.00</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>34.0</td>\n",
       "      <td>20</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>16.5</td>\n",
       "      <td>36.50</td>\n",
       "      <td>19</td>\n",
       "      <td>16.5</td>\n",
       "      <td>16</td>\n",
       "      <td>37.0</td>\n",
       "      <td>20</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>40.00</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>40.00</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>39.0</td>\n",
       "      <td>20</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>19.25</td>\n",
       "      <td>19</td>\n",
       "      <td>37.25</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>36.0</td>\n",
       "      <td>20</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>13.25</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>30.00</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>15.5</td>\n",
       "      <td>36.75</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>12.5</td>\n",
       "      <td>39.0</td>\n",
       "      <td>20</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>19.75</td>\n",
       "      <td>20</td>\n",
       "      <td>19.5</td>\n",
       "      <td>37.50</td>\n",
       "      <td>19</td>\n",
       "      <td>19.75</td>\n",
       "      <td>19</td>\n",
       "      <td>39.5</td>\n",
       "      <td>20</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>32.25</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>36.5</td>\n",
       "      <td>20</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>15.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>13</td>\n",
       "      <td>36.25</td>\n",
       "      <td>16.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>14</td>\n",
       "      <td>31.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>19.5</td>\n",
       "      <td>18</td>\n",
       "      <td>38.25</td>\n",
       "      <td>19.25</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>38.0</td>\n",
       "      <td>20</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>40.00</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>39.0</td>\n",
       "      <td>20</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>19.5</td>\n",
       "      <td>18.75</td>\n",
       "      <td>38.50</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>36.0</td>\n",
       "      <td>20</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID    HW1    HW2    HW3    MT1    HW4    HW5   HW6   MT2   HW7  Final\n",
       "0    1     20      8     12  28.00     20     16    18  24.0    20   32.0\n",
       "1    2     19   18.5   16.5  37.50     19     19  14.5  36.5    20   40.0\n",
       "2    3     16     18     16  24.00     20     18    20  34.0    20   46.0\n",
       "3    4     20     18   16.5  36.50     19   16.5    16  37.0    20   33.0\n",
       "4    5     20     20     20  40.00     20     20    20  40.0    20   50.0\n",
       "5    6     16     20     14  40.00     20     20    20  39.0    20   44.0\n",
       "6    7     19  19.25     19  37.25     20     18    19  36.0    20   48.0\n",
       "7    8  13.25     14     12  30.00     18     20    14  21.0    18   31.0\n",
       "8    9     18     18   15.5  36.75     20     20  12.5  39.0    20   37.0\n",
       "9   10  19.75     20   19.5  37.50     19  19.75    19  39.5    20   47.0\n",
       "10  11      .      .      .  32.25   19.5   19.5  18.5  36.5    20   50.0\n",
       "11  12   15.5   12.5     13  36.25   16.5   18.5    14  31.5  18.5   25.0\n",
       "12  13     19   19.5     18  38.25  19.25     19    15  38.0    20   49.0\n",
       "13  14     20     20     20  40.00     20     20    20  39.0    20   47.0\n",
       "14  15     20   19.5  18.75  38.50     20     20    20  36.0    20   35.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the top of the dataset\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID       float64\n",
       "HW1      float64\n",
       "HW2      float64\n",
       "HW3      float64\n",
       "MT1      float64\n",
       "HW4      float64\n",
       "HW5      float64\n",
       "HW6      float64\n",
       "MT2      float64\n",
       "HW7      float64\n",
       "Final    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deal with missing values via imputation or other method\n",
    "df = data.copy() # create copy of original dataset\n",
    "df.replace(to_replace='.', value=0, inplace=True) # replace missing values with 0\n",
    "df = pd.DataFrame(df, dtype=float) # convert all data types to floats\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deal with missing values via removal - matches what SAS does\n",
    "df_clean = data.copy() # create copy of original dataset\n",
    "df_clean.replace(to_replace='.', value=np.nan, inplace=True) # replace missing values with NaN to remove\n",
    "df_clean.dropna(axis=0, inplace=True) # remove NaNs (axis=0 means it removes rows with missing values, axis=1 would mean columns)\n",
    "df_clean = pd.DataFrame(df_clean, dtype=float) # convert all data types to floats\n",
    "df_clean.shape # look at dimensions of clean data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f06626ae48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f065e26f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure() # create figure space\n",
    "\n",
    "plt.subplot(121) # set up figure space of dimensions 1 x 2. The final '1' in 121 refers to the position in the 1x2 dimensions\n",
    "plt.hist(df['HW1']) # plot histogram of HW1 variable\n",
    "plt.title('With Missing as Zero')\n",
    "\n",
    "plt.subplot(122) # same figure space with 1x2 dimensions, but now the final '2' refers to the second position in this space\n",
    "plt.hist(df_clean['HW1'])\n",
    "plt.title('With Missing Removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'HW1', 'HW2', 'HW3', 'MT1', 'HW4', 'HW5', 'HW6', 'MT2', 'HW7',\n",
       "       'Final'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To produce same results as SAS, use data with missing values removed.\n",
    "df_clean.columns # list columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HW1</th>\n",
       "      <th>HW2</th>\n",
       "      <th>HW3</th>\n",
       "      <th>MT1</th>\n",
       "      <th>HW4</th>\n",
       "      <th>HW5</th>\n",
       "      <th>HW6</th>\n",
       "      <th>MT2</th>\n",
       "      <th>HW7</th>\n",
       "      <th>Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>37.5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>36.5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>36.5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    HW1   HW2   HW3   MT1   HW4   HW5   HW6   MT2   HW7  Final\n",
       "0  20.0   8.0  12.0  28.0  20.0  16.0  18.0  24.0  20.0   32.0\n",
       "1  19.0  18.5  16.5  37.5  19.0  19.0  14.5  36.5  20.0   40.0\n",
       "2  16.0  18.0  16.0  24.0  20.0  18.0  20.0  34.0  20.0   46.0\n",
       "3  20.0  18.0  16.5  36.5  19.0  16.5  16.0  37.0  20.0   33.0\n",
       "4  20.0  20.0  20.0  40.0  20.0  20.0  20.0  40.0  20.0   50.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades = df_clean.iloc[:,1:] # remove ID column\n",
    "grades.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001F0662B5518>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000001F066362A90>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000001F06639AF98>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x000001F0663F6FD0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000001F06643FF98>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000001F06649E8D0>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x000001F0664EB278>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000001F0665181D0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x000001F066583DD8>]], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEKCAYAAAD6q1UVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f065e26240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To get a scatterplot matrix - takes a while to run (also hard to read with lots of variables when printing 'inline')\n",
    "from pandas.plotting import scatter_matrix # don't need to import entire pandas.plotting module, can just import part\n",
    "scatter_matrix(grades[['MT1', 'MT2', 'Final']]) # let's just look at 2 predictors and the response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HW1</th>\n",
       "      <th>HW2</th>\n",
       "      <th>HW3</th>\n",
       "      <th>MT1</th>\n",
       "      <th>HW4</th>\n",
       "      <th>HW5</th>\n",
       "      <th>HW6</th>\n",
       "      <th>MT2</th>\n",
       "      <th>HW7</th>\n",
       "      <th>Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>118.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.129237</td>\n",
       "      <td>17.713983</td>\n",
       "      <td>17.283898</td>\n",
       "      <td>35.309322</td>\n",
       "      <td>18.836864</td>\n",
       "      <td>18.875000</td>\n",
       "      <td>16.919492</td>\n",
       "      <td>34.093220</td>\n",
       "      <td>19.224576</td>\n",
       "      <td>40.029661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.702036</td>\n",
       "      <td>3.609146</td>\n",
       "      <td>3.062894</td>\n",
       "      <td>4.961032</td>\n",
       "      <td>3.302967</td>\n",
       "      <td>2.767089</td>\n",
       "      <td>3.010644</td>\n",
       "      <td>5.698984</td>\n",
       "      <td>1.869226</td>\n",
       "      <td>7.692902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.500000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>34.062500</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>15.062500</td>\n",
       "      <td>31.625000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>38.250000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>19.187500</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>45.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              HW1         HW2         HW3         MT1         HW4         HW5  \\\n",
       "count  118.000000  118.000000  118.000000  118.000000  118.000000  118.000000   \n",
       "mean    18.129237   17.713983   17.283898   35.309322   18.836864   18.875000   \n",
       "std      2.702036    3.609146    3.062894    4.961032    3.302967    2.767089   \n",
       "min      0.000000    0.000000    0.000000   16.000000    0.000000    0.000000   \n",
       "25%     17.500000   17.500000   16.000000   34.062500   19.000000   19.000000   \n",
       "50%     19.000000   19.000000   18.000000   37.000000   20.000000   20.000000   \n",
       "75%     20.000000   20.000000   19.500000   38.250000   20.000000   20.000000   \n",
       "max     20.000000   20.000000   20.000000   40.000000   20.000000   20.000000   \n",
       "\n",
       "              HW6         MT2         HW7       Final  \n",
       "count  118.000000  118.000000  118.000000  118.000000  \n",
       "mean    16.919492   34.093220   19.224576   40.029661  \n",
       "std      3.010644    5.698984    1.869226    7.692902  \n",
       "min      4.250000   11.000000   12.000000   19.000000  \n",
       "25%     15.062500   31.625000   20.000000   35.000000  \n",
       "50%     18.000000   36.000000   20.000000   43.000000  \n",
       "75%     19.187500   38.000000   20.000000   45.375000  \n",
       "max     20.000000   40.000000   20.000000   50.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first get a sense of\n",
    "grades.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10) (118,)\n",
      "(118, 1)\n"
     ]
    }
   ],
   "source": [
    "# Run Linear Regression with Linear Algebra (note in Python slicing is of the form [a,b) i.e. b not included, unlike in R)\n",
    "X_df = grades.iloc[:,:-1] # separte our predictor variables from our response in our data frame (-1 refers to the last)\n",
    "y_df = grades.iloc[:,-1] # extract to the final column\n",
    "\n",
    "# Here we convert our pandas data frames to numpy matrices (aka arrays)\n",
    "# np.hstack() is like cbind() in R\n",
    "X = np.hstack((np.ones((X_df.shape[0],1)),np.array(X_df))) # add column of ones to create design matrix\n",
    "y = np.array(y_df)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "y = y.reshape((y.shape[0],1))\n",
    "print(y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can see there is a difference in the dimensions (118, ) and (118, 1). Sometime you need one format and other times you need the other. It all depends on which functions and methods these arrays are being fed into. \n",
    "- (R,) is a vector or a 1-D array\n",
    "- (R,1) is a 2-D array that happens to only have one column\n",
    "- See this explanation for more details: \n",
    "https://stackoverflow.com/questions/22053050/difference-between-numpy-array-shape-r-1-and-r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to find our estimates of regression coefficients, we perorm the linear algebra operations\n",
    "- np.linalg.inv(A) finds the inverse of a matrix (assuming one exists)\n",
    "- A.T as you can imagine is the transpose of matrix A\n",
    "- A.dot(B) is the product of matrices A and B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b.reshape((X.shape[1],1)) # Regression coefficients match SAS\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(2, 6)\n",
      "[2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "print(range(2,6)) # range is basically made for iterating i.e. it won't print out a list of its elements itself\n",
    "print(list(range(2,6))) # to see what's in this list, use list() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3092948546472596,\n",
       " 2.5116479193676402,\n",
       " 2.185809702691857,\n",
       " 2.4873999172127634,\n",
       " 1.7579489966679791,\n",
       " 1.7349609671485051,\n",
       " 1.3052248126157402,\n",
       " 2.5026047164621446,\n",
       " 1.4804473147006614]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is there multicollinearity?\n",
    "vif = [] # initialize empty list of variance inflation factors to append to\n",
    "\n",
    "for i in range(1,(X[:,1:].shape[1]+1)): \n",
    "    X_vif = np.delete(X, (i), axis=1) # delete ith column (predictor) from design matrix\n",
    "    y_vif = X[:,i:(i+1)] # use ith predictor to regress on the rest\n",
    "    b_vif = np.linalg.inv(X_vif.T.dot(X_vif)).dot(X_vif.T).dot(y_vif) # vif regression coefficient estimates\n",
    "    yhat_vif = X_vif.dot(b_vif) # estimated responses for vif\n",
    "    e_vif = (y_vif - yhat_vif).reshape((y_vif.shape[0],1)) # vif residuals\n",
    "    sse_vif = np.sum(np.power(e_vif,2))  # vif SSE\n",
    "    sst_vif = np.sum(np.power(y_vif - y_vif.mean(),2)) # vif SSTO\n",
    "    ssr_vif = sst_vif - sse_vif # vif SSR\n",
    "    R2_i = ssr_vif / sst_vif # R^2_i\n",
    "    vif_i = 1 / (1 - R2_i) # ith variance inflation factor\n",
    "    vif.append(vif_i) # add to our vif list\n",
    "\n",
    "vif # not really, certainly no variance inflation factors over 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cook's Distance\n",
    "yhat = X.dot(b)\n",
    "e = y - yhat\n",
    "e = e.reshape((e.shape[0],1)) # reshape the residuals into a 2-D array with one column\n",
    "H = X.dot(np.linalg.inv(X.T.dot(X))).dot(X.T) # Hat matrix (projection matrix): yhat = Hy\n",
    "hi = np.reshape(np.diag(H), (np.diag(H).shape[0],1)) # Leverage\n",
    "mse = np.sum(np.power(e,2)) / (X.shape[0] - X.shape[1]) # MSE matches SAS\n",
    "cooksD = (np.power(e,2) / (mse * X.shape[1])) * (hi / np.power((1 - hi),2)) # formula with leverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooksD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f067696e10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8vsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f06667f4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot Cook's Distance by observation - couldn't find a needle plot like in SAS, but this gets the idea across \n",
    "plt.plot(cooksD)\n",
    "plt.title('Cook\\'s Distance')\n",
    "plt.xlabel('Observation Number')\n",
    "plt.ylabel('CooksD')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([74], dtype=int64),) [ 1.1187195]\n"
     ]
    }
   ],
   "source": [
    "np.sum(cooksD>.5) # only one observation has cooks distance higher than .5\n",
    "influential_obs = np.where(cooksD.ravel()>.5) # np.ravel flattens to shape(n,); np.where() is like which() in R\n",
    "print(influential_obs, cooksD[np.where(cooksD>.5)]) # matches SAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.    12.    14.     6.    16.     0.     0.    14.    11.    20.  ]\n",
      " [  1.    18.75  17.    17.    34.5   19.    20.    11.75  31.    20.  ]]\n",
      "[[  1.    18.75  17.    17.    34.5   19.    20.    11.75  31.    20.  ]\n",
      " [  1.    20.    20.    18.    38.    20.    20.    18.    37.    20.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Remove influential observation\n",
    "print(X[74:76,:]) # print observations 74 and 75\n",
    "X_new = np.delete(X, (74), axis=0) # delete 74th observation (row)\n",
    "print(X_new[74:76,:]) # print observations 74 and 75 to make sure it worked\n",
    "y_new = np.delete(y, (74), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.94719582],\n",
       "       [-0.42192619],\n",
       "       [ 0.14680696],\n",
       "       [ 0.09836213],\n",
       "       [ 0.3538559 ],\n",
       "       [ 0.02523333],\n",
       "       [ 0.07564809],\n",
       "       [ 0.13917565],\n",
       "       [ 0.71293909],\n",
       "       [-0.40875589]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-Run Regression without this influential observation.\n",
    "b_new = np.linalg.inv(X_new.T.dot(X_new)).dot(X_new.T).dot(y_new)\n",
    "b_new # matches SAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6920.23076923 6920.23076923\n",
      "F-Statistic:  7.81740502027 p-value:  8.75982886228e-09\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>SS</th>\n",
       "      <th>MS</th>\n",
       "      <th>F</th>\n",
       "      <th>P-val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>9</td>\n",
       "      <td>2745.227</td>\n",
       "      <td>305.025</td>\n",
       "      <td>7.817</td>\n",
       "      <td>8.75983e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Error</th>\n",
       "      <td>107</td>\n",
       "      <td>4175.004</td>\n",
       "      <td>39.019</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>116</td>\n",
       "      <td>6920.231</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Source        SS       MS      F        P-val\n",
       "Model       9  2745.227  305.025  7.817  8.75983e-09\n",
       "Error     107  4175.004   39.019                    \n",
       "Total     116  6920.231                             "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANOVA Table\n",
    "yhat_new = X_new.dot(b_new)\n",
    "e_new = y_new - yhat_new\n",
    "sse = np.sum(np.power(e_new,2))\n",
    "mse = sse / (X_new.shape[0] - X_new.shape[1]) # k = number of predictors, y_new.shape[1] = number of parameters = k + 1\n",
    "ssr = np.sum(np.power(yhat_new - y_new.mean(),2)) # to raise each entry in an array to a power, use np.power()\n",
    "msr = ssr / (X_new.shape[1]-1)\n",
    "sst = np.sum(np.power(y_new - y_new.mean(),2))\n",
    "print(sse + ssr, sst) # check \n",
    "f_stat = msr / mse\n",
    "\n",
    "from scipy.stats import f # let's compute p-value of our F-statistic\n",
    "p_val_f = 1 - f.cdf(f_stat, X_new.shape[1] - 1, X_new.shape[0] - X_new.shape[1])\n",
    "\n",
    "print('F-Statistic: ', f_stat, 'p-value: ', p_val_f)\n",
    "\n",
    "\n",
    "anova_data = [(X_new.shape[1] - 1, np.round(ssr,3), np.round(msr,3), np.round(f_stat,3), p_val_f),\n",
    "             (X_new.shape[0] - X_new.shape[1], np.round(sse,3), np.round(mse,3), '', ''),\n",
    "             (X_new.shape[0] - 1, np.round(sst,3), '', '', '')]\n",
    "\n",
    "anova_table = pd.DataFrame(anova_data, columns=['Source', 'SS', 'MS', 'F', 'P-val'], index=['Model', 'Error', 'Total'])\n",
    "\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate of sigma:  6.246\n"
     ]
    }
   ],
   "source": [
    "# Estimate the standard deviation of the error term\n",
    "s = np.sqrt(mse)\n",
    "print('The estimate of sigma: ', np.round(s,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Sqauared:  0.396695850387 Adjusted R-Squared:  0.345950641541\n"
     ]
    }
   ],
   "source": [
    "# Compute R^2 and Adjusted R^2\n",
    "R2 = ssr / sst\n",
    "R2_adj = 1 - (1 - R2) * (X_new.shape[0] - 1) / (X_new.shape[0] - X_new.shape[1])\n",
    "print('R-Sqauared: ', R2, 'Adjusted R-Squared: ', R2_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter Estimate</th>\n",
       "      <th>Standard Error</th>\n",
       "      <th>t Value</th>\n",
       "      <th>Pr &gt; |t|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>9.947196</td>\n",
       "      <td>8.422358</td>\n",
       "      <td>1.181046</td>\n",
       "      <td>0.240203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HW1</th>\n",
       "      <td>-0.421926</td>\n",
       "      <td>0.332539</td>\n",
       "      <td>-1.268802</td>\n",
       "      <td>0.207265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HW2</th>\n",
       "      <td>0.146807</td>\n",
       "      <td>0.279239</td>\n",
       "      <td>0.525740</td>\n",
       "      <td>0.600157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HW3</th>\n",
       "      <td>0.098362</td>\n",
       "      <td>0.279060</td>\n",
       "      <td>0.352477</td>\n",
       "      <td>0.725174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT1</th>\n",
       "      <td>0.353856</td>\n",
       "      <td>0.184066</td>\n",
       "      <td>1.922436</td>\n",
       "      <td>0.057210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HW4</th>\n",
       "      <td>0.025233</td>\n",
       "      <td>0.252655</td>\n",
       "      <td>0.099873</td>\n",
       "      <td>0.920632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HW5</th>\n",
       "      <td>0.075648</td>\n",
       "      <td>0.321975</td>\n",
       "      <td>0.234950</td>\n",
       "      <td>0.814696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HW6</th>\n",
       "      <td>0.139176</td>\n",
       "      <td>0.220430</td>\n",
       "      <td>0.631381</td>\n",
       "      <td>0.529138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT2</th>\n",
       "      <td>0.712939</td>\n",
       "      <td>0.163753</td>\n",
       "      <td>4.353741</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HW7</th>\n",
       "      <td>-0.408756</td>\n",
       "      <td>0.397344</td>\n",
       "      <td>-1.028721</td>\n",
       "      <td>0.305931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Parameter Estimate  Standard Error   t Value  Pr > |t|\n",
       "Intercept            9.947196        8.422358  1.181046  0.240203\n",
       "HW1                 -0.421926        0.332539 -1.268802  0.207265\n",
       "HW2                  0.146807        0.279239  0.525740  0.600157\n",
       "HW3                  0.098362        0.279060  0.352477  0.725174\n",
       "MT1                  0.353856        0.184066  1.922436  0.057210\n",
       "HW4                  0.025233        0.252655  0.099873  0.920632\n",
       "HW5                  0.075648        0.321975  0.234950  0.814696\n",
       "HW6                  0.139176        0.220430  0.631381  0.529138\n",
       "MT2                  0.712939        0.163753  4.353741  0.000031\n",
       "HW7                 -0.408756        0.397344 -1.028721  0.305931"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variance-Covariance Matrix\n",
    "cov_matrix = mse * np.linalg.inv(X_new.T.dot(X_new))\n",
    "\n",
    "# Standard errors of regression coefficients\n",
    "standard_errors = np.sqrt(np.diag(cov_matrix))\n",
    "\n",
    "# t-statistics\n",
    "t_stat = b_new.ravel() / standard_errors # with numpy operations, you can use np.operation(A) or A.operation()\n",
    "\n",
    "# P-values\n",
    "from scipy.stats import t\n",
    "p_vals = 2*(1 - t.cdf(np.abs(t_stat), X_new.shape[0] - X_new.shape[1]))\n",
    "\n",
    "# Create Parameter Estimates table as in SAS\n",
    "param_est_data = np.hstack((b_new, standard_errors.reshape((X_new.shape[1],1)), t_stat.reshape((X_new.shape[1],1)), p_vals.reshape((X_new.shape[1],1))))\n",
    "var_list = list(grades.columns[:-1])\n",
    "var_list.insert(0, 'Intercept')\n",
    "param_est_table = pd.DataFrame(param_est_data, columns=['Parameter Estimate', 'Standard Error', 't Value', 'Pr > |t|'], index=var_list)\n",
    "param_est_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lower</th>\n",
       "      <th>Upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>-6.749147</td>\n",
       "      <td>26.643538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HW1</th>\n",
       "      <td>-1.081146</td>\n",
       "      <td>0.237294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HW2</th>\n",
       "      <td>-0.406752</td>\n",
       "      <td>0.700366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HW3</th>\n",
       "      <td>-0.454842</td>\n",
       "      <td>0.651566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT1</th>\n",
       "      <td>-0.011034</td>\n",
       "      <td>0.718746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HW4</th>\n",
       "      <td>-0.475626</td>\n",
       "      <td>0.526093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HW5</th>\n",
       "      <td>-0.562629</td>\n",
       "      <td>0.713926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HW6</th>\n",
       "      <td>-0.297802</td>\n",
       "      <td>0.576153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MT2</th>\n",
       "      <td>0.388317</td>\n",
       "      <td>1.037561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HW7</th>\n",
       "      <td>-1.196443</td>\n",
       "      <td>0.378932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Lower      Upper\n",
       "Intercept -6.749147  26.643538\n",
       "HW1       -1.081146   0.237294\n",
       "HW2       -0.406752   0.700366\n",
       "HW3       -0.454842   0.651566\n",
       "MT1       -0.011034   0.718746\n",
       "HW4       -0.475626   0.526093\n",
       "HW5       -0.562629   0.713926\n",
       "HW6       -0.297802   0.576153\n",
       "MT2        0.388317   1.037561\n",
       "HW7       -1.196443   0.378932"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confidence Intervals for Regression Coefficients\n",
    "# ppf (percent point function) is the inverse cdf\n",
    "upper = b_new + t.ppf(.975, X_new.shape[0] - X_new.shape[1]) * standard_errors.reshape((X_new.shape[1],1))\n",
    "lower = b_new - t.ppf(.975, X_new.shape[0] - X_new.shape[1]) * standard_errors.reshape((X_new.shape[1],1))\n",
    "CI_b_data = np.hstack((lower, upper))\n",
    "CI_b = pd.DataFrame(CI_b_data, columns=['Lower', 'Upper'], index=var_list)\n",
    "CI_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodels API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.94719582 -0.42192619  0.14680696  0.09836213  0.3538559   0.02523333\n",
      "  0.07564809  0.13917565  0.71293909 -0.40875589]\n",
      "[[ 9.94719582]\n",
      " [-0.42192619]\n",
      " [ 0.14680696]\n",
      " [ 0.09836213]\n",
      " [ 0.3538559 ]\n",
      " [ 0.02523333]\n",
      " [ 0.07564809]\n",
      " [ 0.13917565]\n",
      " [ 0.71293909]\n",
      " [-0.40875589]]\n"
     ]
    }
   ],
   "source": [
    "# Now using statsmodels api \n",
    "mlr = sm.OLS(y_new, X_new).fit() # OLS method of the statsmodels module (nicknamed 'sm')\n",
    "print(mlr.params) # access the estimated regression coefficients via the parameters attribute\n",
    "print(b_new) # check (from previous estimation with numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   7.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 26 Jan 2019</td> <th>  Prob (F-statistic):</th> <td>8.76e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:28:54</td>     <th>  Log-Likelihood:    </th> <td> -375.14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   117</td>      <th>  AIC:               </th> <td>   770.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   107</td>      <th>  BIC:               </th> <td>   797.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     9</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    9.9472</td> <td>    8.422</td> <td>    1.181</td> <td> 0.240</td> <td>   -6.749</td> <td>   26.644</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.4219</td> <td>    0.333</td> <td>   -1.269</td> <td> 0.207</td> <td>   -1.081</td> <td>    0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.1468</td> <td>    0.279</td> <td>    0.526</td> <td> 0.600</td> <td>   -0.407</td> <td>    0.700</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0984</td> <td>    0.279</td> <td>    0.352</td> <td> 0.725</td> <td>   -0.455</td> <td>    0.652</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.3539</td> <td>    0.184</td> <td>    1.922</td> <td> 0.057</td> <td>   -0.011</td> <td>    0.719</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0252</td> <td>    0.253</td> <td>    0.100</td> <td> 0.921</td> <td>   -0.476</td> <td>    0.526</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.0756</td> <td>    0.322</td> <td>    0.235</td> <td> 0.815</td> <td>   -0.563</td> <td>    0.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    0.1392</td> <td>    0.220</td> <td>    0.631</td> <td> 0.529</td> <td>   -0.298</td> <td>    0.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    0.7129</td> <td>    0.164</td> <td>    4.354</td> <td> 0.000</td> <td>    0.388</td> <td>    1.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>   -0.4088</td> <td>    0.397</td> <td>   -1.029</td> <td> 0.306</td> <td>   -1.196</td> <td>    0.379</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>20.950</td> <th>  Durbin-Watson:     </th> <td>   1.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  26.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.003</td> <th>  Prob(JB):          </th> <td>2.06e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.159</td> <th>  Cond. No.          </th> <td>1.01e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.01e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.397\n",
       "Model:                            OLS   Adj. R-squared:                  0.346\n",
       "Method:                 Least Squares   F-statistic:                     7.817\n",
       "Date:                Sat, 26 Jan 2019   Prob (F-statistic):           8.76e-09\n",
       "Time:                        01:28:54   Log-Likelihood:                -375.14\n",
       "No. Observations:                 117   AIC:                             770.3\n",
       "Df Residuals:                     107   BIC:                             797.9\n",
       "Df Model:                           9                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          9.9472      8.422      1.181      0.240      -6.749      26.644\n",
       "x1            -0.4219      0.333     -1.269      0.207      -1.081       0.237\n",
       "x2             0.1468      0.279      0.526      0.600      -0.407       0.700\n",
       "x3             0.0984      0.279      0.352      0.725      -0.455       0.652\n",
       "x4             0.3539      0.184      1.922      0.057      -0.011       0.719\n",
       "x5             0.0252      0.253      0.100      0.921      -0.476       0.526\n",
       "x6             0.0756      0.322      0.235      0.815      -0.563       0.714\n",
       "x7             0.1392      0.220      0.631      0.529      -0.298       0.576\n",
       "x8             0.7129      0.164      4.354      0.000       0.388       1.038\n",
       "x9            -0.4088      0.397     -1.029      0.306      -1.196       0.379\n",
       "==============================================================================\n",
       "Omnibus:                       20.950   Durbin-Watson:                   1.843\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               26.186\n",
       "Skew:                          -1.003   Prob(JB):                     2.06e-06\n",
       "Kurtosis:                       4.159   Cond. No.                     1.01e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.01e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlr.summary() # similar to summary(lm(y~.)) and SAS output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One disadvantage to the above output is that since the data was transformed from dataframes to numpy arrays, the data lost their variable names.\n",
    "- We can fix this by inputting a list of variables into the summary() method as we will see below with Backward Elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.40202824e-01   2.07264991e-01   6.00157003e-01   7.25174160e-01\n",
      "   5.72099261e-02   9.20632305e-01   8.14696327e-01   5.29137889e-01\n",
      "   3.07472578e-05   3.05930537e-01]\n"
     ]
    }
   ],
   "source": [
    "pvals_sm = mlr.pvalues\n",
    "print(pvals_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.375</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 26 Jan 2019</td> <th>  Prob (F-statistic):</th> <td>2.40e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:28:59</td>     <th>  Log-Likelihood:    </th> <td> -377.24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   117</td>      <th>  AIC:               </th> <td>   760.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   114</td>      <th>  BIC:               </th> <td>   768.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    4.0542</td> <td>    4.673</td> <td>    0.868</td> <td> 0.387</td> <td>   -5.203</td> <td>   13.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.3530</td> <td>    0.147</td> <td>    2.396</td> <td> 0.018</td> <td>    0.061</td> <td>    0.645</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.6835</td> <td>    0.129</td> <td>    5.297</td> <td> 0.000</td> <td>    0.428</td> <td>    0.939</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>21.402</td> <th>  Durbin-Watson:     </th> <td>   1.831</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  26.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.058</td> <th>  Prob(JB):          </th> <td>1.81e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.972</td> <th>  Cond. No.          </th> <td>    408.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.375\n",
       "Model:                            OLS   Adj. R-squared:                  0.364\n",
       "Method:                 Least Squares   F-statistic:                     34.14\n",
       "Date:                Sat, 26 Jan 2019   Prob (F-statistic):           2.40e-12\n",
       "Time:                        01:28:59   Log-Likelihood:                -377.24\n",
       "No. Observations:                 117   AIC:                             760.5\n",
       "Df Residuals:                     114   BIC:                             768.8\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          4.0542      4.673      0.868      0.387      -5.203      13.311\n",
       "x1             0.3530      0.147      2.396      0.018       0.061       0.645\n",
       "x2             0.6835      0.129      5.297      0.000       0.428       0.939\n",
       "==============================================================================\n",
       "Omnibus:                       21.402   Durbin-Watson:                   1.831\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               26.440\n",
       "Skew:                          -1.058   Prob(JB):                     1.81e-06\n",
       "Kurtosis:                       3.972   Cond. No.                         408.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Backward Elimination (BE)\n",
    "slstay = .10\n",
    "X_BE = X_new\n",
    "while pvals_sm[1:][np.argmax(pvals_sm[1:])] > slstay: # don't want to eliminate the intercept\n",
    "    \n",
    "    # delete entire column corresponding to least significant predictor\n",
    "    X_BE_temp = np.delete(X_BE[:,1:], np.argmax(pvals_sm[1:]), axis=1) \n",
    "    \n",
    "    # reattach column of 1s for design matrix\n",
    "    X_BE = np.hstack((np.ones((X_new.shape[0],1)), X_BE_temp))\n",
    "    \n",
    "    # re-fit new regression model\n",
    "    mlr_BE = sm.OLS(y_new, X_BE).fit()\n",
    "    \n",
    "    # extract new p-values\n",
    "    pvals_sm = mlr_BE.pvalues\n",
    "\n",
    "mlr_BE.summary() # matches SAS - x1 and x2 are MT1 and MT2 (but only from looking at SAS output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see from the above output, x1 and x2 show up as the predictor names, which isn't terribly helpful.\n",
    "- This is fixed in the code below by deleting the variables from a variable list in the same iteration as deleting the columns in our data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "HW4\n",
      "4\n",
      "HW5\n",
      "2\n",
      "HW3\n",
      "3\n",
      "HW6\n",
      "1\n",
      "HW2\n",
      "3\n",
      "HW7\n",
      "0\n",
      "HW1\n",
      "['MT1', 'MT2']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.375</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 26 Jan 2019</td> <th>  Prob (F-statistic):</th> <td>2.40e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:29:02</td>     <th>  Log-Likelihood:    </th> <td> -377.24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   117</td>      <th>  AIC:               </th> <td>   760.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   114</td>      <th>  BIC:               </th> <td>   768.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    4.0542</td> <td>    4.673</td> <td>    0.868</td> <td> 0.387</td> <td>   -5.203</td> <td>   13.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MT1</th>       <td>    0.3530</td> <td>    0.147</td> <td>    2.396</td> <td> 0.018</td> <td>    0.061</td> <td>    0.645</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MT2</th>       <td>    0.6835</td> <td>    0.129</td> <td>    5.297</td> <td> 0.000</td> <td>    0.428</td> <td>    0.939</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>21.402</td> <th>  Durbin-Watson:     </th> <td>   1.831</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  26.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.058</td> <th>  Prob(JB):          </th> <td>1.81e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.972</td> <th>  Cond. No.          </th> <td>    408.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.375\n",
       "Model:                            OLS   Adj. R-squared:                  0.364\n",
       "Method:                 Least Squares   F-statistic:                     34.14\n",
       "Date:                Sat, 26 Jan 2019   Prob (F-statistic):           2.40e-12\n",
       "Time:                        01:29:02   Log-Likelihood:                -377.24\n",
       "No. Observations:                 117   AIC:                             760.5\n",
       "Df Residuals:                     114   BIC:                             768.8\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      4.0542      4.673      0.868      0.387      -5.203      13.311\n",
       "MT1            0.3530      0.147      2.396      0.018       0.061       0.645\n",
       "MT2            0.6835      0.129      5.297      0.000       0.428       0.939\n",
       "==============================================================================\n",
       "Omnibus:                       21.402   Durbin-Watson:                   1.831\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               26.440\n",
       "Skew:                          -1.058   Prob(JB):                     1.81e-06\n",
       "Kurtosis:                       3.972   Cond. No.                         408.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Backward Elimination (BE) - with correct variables in output\n",
    "slstay = .10 # threshold for staying in the model\n",
    "X_BE = X_new\n",
    "var_list = list(grades.columns[:-1])\n",
    "pvals_sm = mlr.pvalues\n",
    "var_list_BE = var_list\n",
    "\n",
    "while pvals_sm[1:][np.argmax(pvals_sm[1:])] > slstay: # don't want to eliminate the intercept\n",
    "    \n",
    "    # delete entire column corresponding to least significant predictor\n",
    "    X_BE_temp = np.delete(X_BE[:,1:], np.argmax(pvals_sm[1:]), axis=1) \n",
    "    print(np.argmax(pvals_sm[1:])) # print index to be deleted\n",
    "    print(var_list_BE[np.argmax(pvals_sm[1:])]) # print predictor to be deleted\n",
    "    del var_list_BE[np.argmax(pvals_sm[1:])] # delete corresponding variable from variable list\n",
    "    \n",
    "    \n",
    "    # reattach column of 1s for design matrix\n",
    "    X_BE = np.hstack((np.ones((X_new.shape[0],1)), X_BE_temp))\n",
    "    \n",
    "    # re-fit new regression model\n",
    "    mlr_BE = sm.OLS(y_new, X_BE).fit()\n",
    "    \n",
    "    # extract new p-values\n",
    "    pvals_sm = mlr_BE.pvalues\n",
    "\n",
    "print(var_list_BE) # predictors left after Backward Elimination   \n",
    "var_list_BE = ['Intercept'] + var_list_BE # insert the intercept variable name at beginning of variable list\n",
    "mlr_BE.summary(xname=var_list_BE) # matches SAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Better yet, let's create a Backward Elimination function for future use.\n",
    "\n",
    "### Notes: \n",
    "### - X_BE is entire design matrix (including column of ones) - type: numpy matrix\n",
    "### - var_list_BE is list of predictor variables (NOT including intercept) - type: list\n",
    "### - pvals_sm is list of all p-values (including intercept as 1st) - type: list\n",
    "\n",
    "def backward_elimination(X_BE, var_list_BE, pvals_sm, slstay=.1):\n",
    "    \n",
    "    while pvals_sm[1:][np.argmax(pvals_sm[1:])] > slstay: # don't want to eliminate the intercept\n",
    "    \n",
    "        # delete entire column corresponding to least significant predictor\n",
    "        X_BE_temp = np.delete(X_BE[:,1:], np.argmax(pvals_sm[1:]), axis=1) \n",
    "        print(np.argmax(pvals_sm[1:])) # print index to be deleted\n",
    "        print(var_list_BE[np.argmax(pvals_sm[1:])]) # print predictor to be deleted\n",
    "        del var_list_BE[np.argmax(pvals_sm[1:])] # delete corresponding variable from variable list\n",
    "    \n",
    "        # reattach column of 1s for design matrix\n",
    "        X_BE = np.hstack((np.ones((X_new.shape[0],1)), X_BE_temp))\n",
    "    \n",
    "        # re-fit new regression model\n",
    "        mlr_BE = sm.OLS(y_new, X_BE).fit()\n",
    "    \n",
    "        # extract new p-values\n",
    "        pvals_sm = mlr_BE.pvalues\n",
    "\n",
    "    print('Predictors Left: ', var_list_BE) # predictors left after Backward Elimination   \n",
    "    var_list_BE = ['Intercept'] + var_list_BE # insert the intercept variable name at beginning of variable list\n",
    "    print(mlr_BE.summary(xname=var_list_BE)) # matches SAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "HW4\n",
      "4\n",
      "HW5\n",
      "2\n",
      "HW3\n",
      "3\n",
      "HW6\n",
      "1\n",
      "HW2\n",
      "3\n",
      "HW7\n",
      "0\n",
      "HW1\n",
      "Predictors Left:  ['MT1', 'MT2']\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.375\n",
      "Model:                            OLS   Adj. R-squared:                  0.364\n",
      "Method:                 Least Squares   F-statistic:                     34.14\n",
      "Date:                Sat, 26 Jan 2019   Prob (F-statistic):           2.40e-12\n",
      "Time:                        01:29:06   Log-Likelihood:                -377.24\n",
      "No. Observations:                 117   AIC:                             760.5\n",
      "Df Residuals:                     114   BIC:                             768.8\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      4.0542      4.673      0.868      0.387      -5.203      13.311\n",
      "MT1            0.3530      0.147      2.396      0.018       0.061       0.645\n",
      "MT2            0.6835      0.129      5.297      0.000       0.428       0.939\n",
      "==============================================================================\n",
      "Omnibus:                       21.402   Durbin-Watson:                   1.831\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               26.440\n",
      "Skew:                          -1.058   Prob(JB):                     1.81e-06\n",
      "Kurtosis:                       3.972   Cond. No.                         408.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "slstay = .10 \n",
    "X_BE = X_new\n",
    "var_list = list(grades.columns[:-1])\n",
    "pvals_sm = mlr.pvalues\n",
    "var_list_BE = var_list\n",
    "\n",
    "backward_elimination(X_BE=X_BE, var_list_BE=var_list_BE, pvals_sm=pvals_sm, slstay=slstay) # success!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sci-kit Learn (Sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlr_sklearn = LinearRegression() # create a linear regression instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlr_sklearn.fit(X_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.8213096018\n",
      "[-0.61258545  0.45899665  0.0633583   0.31844575 -0.24305647 -0.37194968\n",
      "  0.20270489  0.62366102 -0.06497343]\n",
      "[[ 18.8213096   -0.61258545   0.45899665   0.0633583    0.31844575\n",
      "   -0.24305647  -0.37194968   0.20270489   0.62366102  -0.06497343]]\n"
     ]
    }
   ],
   "source": [
    "# Extract estimates for coefficients \n",
    "print(mlr_sklearn.intercept_)\n",
    "print(mlr_sklearn.coef_)\n",
    "print(b.T) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356861643233\n"
     ]
    }
   ],
   "source": [
    "# Find R^2 of our model\n",
    "print(mlr_sklearn.score(X_df, y_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:  [ 9.94719582]\n",
      "Estimated Slopes:  [[ 0.         -0.42192619  0.14680696  0.09836213  0.3538559   0.02523333\n",
      "   0.07564809  0.13917565  0.71293909 -0.40875589]]\n",
      "Numpy Estimated Coefficients:  [[ 9.94719582 -0.42192619  0.14680696  0.09836213  0.3538559   0.02523333\n",
      "   0.07564809  0.13917565  0.71293909 -0.40875589]]\n",
      "R^2:  0.396695850387\n"
     ]
    }
   ],
   "source": [
    "# what if we delete that influential observation just as we did with Numpy?\n",
    "mlr2_sklearn = LinearRegression()\n",
    "mlr2_sklearn.fit(X_new, y_new)\n",
    "print('Intercept: ', mlr2_sklearn.intercept_)\n",
    "print('Estimated Slopes: ', mlr2_sklearn.coef_)\n",
    "print('Numpy Estimated Coefficients: ', b_new.T) # check - success\n",
    "print('R^2: ', mlr2_sklearn.score(X_new, y_new)) # check - success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, we can feed either data frames or numpy arrays into this sklearn model.\n",
    "- Another thing to note is that we fed into this sklearn version of linear regression the design matrix with the column of 1s already in it. You see that it treats this column of 1s as a variable, estimates its slope as 0, and finds its own intercept which turns out to be the same as if we didn't feed sklearn the column of 1s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:  [ 9.94719582]\n",
      "Estimated Slopes:  [[-0.42192619  0.14680696  0.09836213  0.3538559   0.02523333  0.07564809\n",
      "   0.13917565  0.71293909 -0.40875589]]\n",
      "R^2:  0.396695850387\n"
     ]
    }
   ],
   "source": [
    "# Let's demonstrate the above note\n",
    "X_new_no1s = X_new[:,1:]\n",
    "mlr2_alt_sklearn = LinearRegression()\n",
    "mlr2_alt_sklearn.fit(X_new_no1s, y_new)\n",
    "print('Intercept: ', mlr2_alt_sklearn.intercept_)\n",
    "print('Estimated Slopes: ', mlr2_alt_sklearn.coef_)\n",
    "print('R^2: ', mlr2_alt_sklearn.score(X_new_no1s, y_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use this framework to do some predictive modeling. \n",
    "- In machine learning, we generally split the data into a training set and testing set. We feed the training data into our model to calibrate our model and get the estimated coefficients for the slopes and intercept. Once we have our model, we want to see how it performs on data, our model hasn't seen before. The purpose of this, is to assess how well our model generalizes to new data. \n",
    "- First, we will generate new data for 20 students not in our original dataset. \n",
    "- Second, we will test our model we trained on our original data on these new observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's re-train our model but now we will remove the bottom 15 data points of the data and use that as a test set.\n",
    "\n",
    "X_train = X_new[:-20, :]\n",
    "y_train = y_new[:-20, :]\n",
    "\n",
    "X_test = X_new[-20:, :]\n",
    "y_test = y_new[-20:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.412296244066155"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlr_ML = LinearRegression()\n",
    "mlr_ML.fit(X_train, y_train)\n",
    "\n",
    "mlr_ML.score(X_train, y_train) # R^2 for the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22129004667971397"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlr_ML.score(X_test, y_test) # not a great R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  41.7602680217\n",
      "RMSE:  6.46221850619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_predict = mlr_ML.predict(X_test)\n",
    "mlr_ML_mse = mean_squared_error(y_predict, y_test)\n",
    "print('MSE: ', mlr_ML_mse)\n",
    "print('RMSE: ', np.sqrt(mlr_ML_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that this MSE (and therefore RMSE) is slightly different from how we normally compute it in OLS regression. \n",
    "$$MSE(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "- Here, we don't use $n-k-1$ in the denominator (accounting for model complexity). I'm not 100% sure why, other than maybe because in machine learning, you tend to work with very large datasets, so adjusting the denominator by the number of estimated parameters won't in general make much of a difference (at least if n is much larger than k). Also another reason could be that this MSE can be used to compare several different kinds of models, not just OLS, so perhaps the ML practitioners wanted some kind of uniformity among error measures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You'll notice, we haven't used any qualitative variables in this homework assignment. Dummy variables can be easily generated for these variables using the pd.get_dummies() function in pandas. It has the functionality to creat dummies with both k-1 levels and k levels. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
